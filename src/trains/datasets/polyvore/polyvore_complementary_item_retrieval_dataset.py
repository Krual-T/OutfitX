import pathlib
import json
import random
from collections import Counter, defaultdict

from typing import Literal, List, cast
from unittest import TestCase

import pandas as pd
import torch

from src.models.datatypes import FashionItem, OutfitComplementaryItemRetrievalTask
from .polyvore_item_dataset import PolyvoreItemDataset
from src.project_settings.info import PROJECT_DIR as ROOT_DIR


class PolyvoreComplementaryItemRetrievalDataset(PolyvoreItemDataset):
    def __init__(
        self,
        polyvore_type: Literal['nondisjoint', 'disjoint'] = 'nondisjoint',
        mode: Literal['train', 'valid', 'test'] = 'train',
        dataset_dir: pathlib.Path = ROOT_DIR / 'datasets' / 'polyvore',
        metadata: dict = None,
        embedding_dict: dict = None,
        load_image: bool = False,
        negative_sample_mode: Literal['easy', 'hard'] = 'easy',
        negative_sample_k: int = 10
    ):
        super().__init__(
            dataset_dir=dataset_dir,
            metadata=metadata,
            embedding_dict=embedding_dict,
            load_image=load_image
        )
        self.polyvore_type = polyvore_type
        self.mode = mode
        self.large_category_threshold = 0 if mode == 'train' else 3000
        self.negative_sample_fine_grained = 'semantic_category' if negative_sample_mode == 'easy' else 'category_id'
        self.negative_sample_k = negative_sample_k

        self.large_categories = self.__get_large_categories()
        self.cir_dataset = self.__load_split_dataset()
        self.negative_pool = self.__build_negative_pool()
        self.candidate_pools = self.__build_candidate_pool() if self.mode != 'train' else {}


    def __len__(self):
        return len(self.cir_dataset)

    def __getitem__(self, index):
        #è·å– outfit positive negativeçš„item_id
        item_ids = list(self.cir_dataset[index]['item_ids'])
        positive_idx_list = self.cir_dataset[index]['positive_idx_list']
        positive_idx = random.choice(positive_idx_list)
        positive_item_id = item_ids.pop(positive_idx)
        negative_item_ids = self.__get_negative_sample(positive_item_id)
        random.shuffle(item_ids)
        # æ„å»ºquery
        query: OutfitComplementaryItemRetrievalTask = OutfitComplementaryItemRetrievalTask(
            outfit=[self.get_item(item_id) for item_id in item_ids],
            target_item=self.get_item(positive_item_id)
        )
        # è·å– negative_items_embedding
        negative_items_embedding = [
            self.embedding_dict[item_id] for item_id in negative_item_ids
        ]
        return query, negative_items_embedding

    def __load_split_dataset(self) -> List[dict]:
        path = self.dataset_dir / self.polyvore_type / f'{self.mode}.json'
        with open(path, 'r',encoding='utf-8') as f:
            raw_data = json.load(f)
        result = []
        for outfit in raw_data:
            item_ids = outfit["item_ids"]
            pos_idx_list = [
                i for i, item_id in enumerate(item_ids)
                if self.metadata[item_id]["category_id"] in self.large_categories
            ]
            if pos_idx_list:
                result.append({
                    "item_ids": item_ids,
                    "positive_idx_list": pos_idx_list
                })
        return result

    def __get_large_categories(self) -> set:
        counts = Counter(
            item["category_id"] for item in self.metadata.values() if "category_id" in item
        )
        return {cid for cid, count in counts.items() if count >= self.large_category_threshold}


    def __build_negative_pool(self):
        negative_pool = defaultdict(list)
        for item in self.metadata.values():
            sample_key = item[self.negative_sample_fine_grained]
            negative_pool[sample_key].append(item['item_id'])
        return negative_pool

    def __get_negative_sample(self, item_id) -> List[int]:
        k = self.negative_sample_k
        item_meta = self.metadata[item_id]
        sample_key = item_meta[self.negative_sample_fine_grained]
        pool = self.negative_pool.get(sample_key, [])
        filtered = [x for x in pool if x != item_id]
        if len(filtered) < k:
            print(f"âš ï¸ ç±»åˆ« {self.negative_sample_fine_grained} è´Ÿæ ·æœ¬ä¸è¶³ {k} ä¸ªï¼Œä»…æœ‰ {len(filtered)} ä¸ª")
        return random.sample(filtered, k) if len(filtered) >= k else filtered

    def __build_candidate_pool(self) -> dict:
        candidate_max_size = 3000
        candidate_pool = {}
        # set item_idé›†åˆ
        split_item_ids = {iid for sample in self.cir_dataset for iid in sample["item_ids"]}
        category_to_all = defaultdict(list)
        category_to_split = defaultdict(set)

        for item_id, item in self.metadata.items():
            cid = item.get("category_id")
            if cid in self.large_categories:
                category_to_all[cid].append(item_id)
                if item_id in split_item_ids:
                    category_to_split[cid].add(item_id)

        for cid in self.large_categories:
            used = list(category_to_split[cid])
            replenish = list(set(category_to_all[cid]) - set(used))
            random.shuffle(replenish)
            total = used + replenish[:max(0, candidate_max_size - len(used))]
            total = total[:candidate_max_size]
            random.shuffle(total)

            index_map = {item_id: idx for idx, item_id in enumerate(total)}

            # âœ… embedding tensor
            try:
                embeddings = torch.stack([
                    torch.tensor(self.embedding_dict[item_id],dtype=torch.float)
                    for item_id in total
                ])
            except KeyError as e:
                print(f"âš ï¸ embedding_dict ç¼ºå¤± item_id: {e}")
                raise e

            candidate_pool[cid] = {
                'item_ids': total,
                'index': index_map,
                'embeddings': embeddings  # shape: [3000, D]
            }

        print(f"âœ… å€™é€‰æ± æ„å»ºå®Œæ¯•ï¼šæ¯ç±» {candidate_max_size} ä¸ª")
        return candidate_pool

    @staticmethod
    def train_collate_fn(batch):
        """
        å¼ƒç”¨ï¼Œå› ä¸ºåœ¨processorä¸­å·²ç»å¤„ç†äº†
        :param batch:
        :return:
        """
        query_iter, neg_items_emb_iter = zip(*batch)
        queries = [query for query in query_iter]
        pos_item_embeddings = torch.stack([
            torch.tensor(
                query.target_item.embedding,
                dtype=torch.float,
            )
            for query in queries
        ])
        neg_items_emb_tensors = torch.stack([
            torch.stack([
                torch.tensor(
                    item_emb,
                    dtype=torch.float,
                )
                for item_emb in neg_items_emb
            ])
            for neg_items_emb in neg_items_emb_iter
        ])

        return queries, pos_item_embeddings, neg_items_emb_tensors
    @staticmethod
    def valid_collate_fn(batch):
        """
        å¼ƒç”¨ï¼Œå› ä¸ºåœ¨processorä¸­å·²ç»å¤„ç†äº†
        :param batch:
        :return:
        """
        query_iter, neg_items_emb_iter = zip(*batch)
        queries = [query for query in query_iter]
        pos_item_ = {
            'ids': [
                query.target_item.item_id for query in queries
            ],
            'embeddings': torch.stack([
                torch.tensor(
                    query.target_item.embedding,
                    dtype=torch.float
                )
                for query in queries
            ])
        }
        neg_items_emb_tensors = torch.stack([
            torch.stack([
                torch.tensor(
                    item_emb,
                    dtype=torch.float
                )
                for item_emb in neg_items_emb
            ])
            for neg_items_emb in neg_items_emb_iter
        ])
        return queries, pos_item_, neg_items_emb_tensors
    @staticmethod
    def test_collate_fn(batch):
        """
        å¼ƒç”¨ï¼Œå› ä¸ºåœ¨processorä¸­å·²ç»å¤„ç†äº†
        :param batch:
        :return:
        """
        query_iter, _ = zip(*batch)
        queries = [query for query in query_iter]
        pos_item_ids = [query.target_item.item_id for query in queries]
        return queries, pos_item_ids

class Test(TestCase):
    def test_check_semantic_category(self):
        import json
        from collections import Counter

        def analyze_semantic_categories(json_path):
            with open(json_path, "r", encoding='utf-8') as f:
                item_metadata = json.load(f)

            semantic_categories = []
            missing_count = 0
            non_string_count = 0

            for item in item_metadata:
                category = item.get("category_id")
                if category is None:
                    missing_count += 1
                elif not isinstance(category, int):
                    non_string_count += 1
                else:
                    semantic_categories.append(category)

            total_items = len(item_metadata)
            unique_categories = set(semantic_categories)
            category_counts = Counter(semantic_categories)

            print("ğŸ” åˆ†æç»“æœï¼š")
            print(f"æ€» item æ•°é‡: {total_items}")
            print(f"ç¼ºå¤± semantic_category çš„æ•°é‡: {missing_count}")
            print(f"semantic_category éå­—ç¬¦ä¸²çš„æ•°é‡: {non_string_count}")
            print(f"å”¯ä¸€ semantic_category ç±»åˆ«æ•°é‡: {len(unique_categories)}")
            print("ğŸ¯ æ‰€æœ‰ç±»åˆ«å¦‚ä¸‹ï¼š")
            for cat in sorted(unique_categories):
                print(f"  - {cat}")
            print("ğŸ“Š ç±»åˆ«å‡ºç°é¢‘ç‡ï¼ˆTop 10ï¼‰ï¼š")
            least_common = category_counts.most_common()[::-1][:10]
            for cat, count in least_common:
                print(f"  {cat}: {count}")
            # for cat, count in category_counts.most_common(-10):
            #     print(f"  {cat}: {count}")

        # ä½¿ç”¨æ–¹æ³•ï¼š
        # analyze_semantic_categories("polyvore_item_metadata.json")

        metadata_path = ROOT_DIR / 'datasets' / 'polyvore' / 'item_metadata.json'
        analyze_semantic_categories(metadata_path)

    def test_build_candidate_pool(self):
        """
        æ ¹æ®å¤§ç±»æ„å»ºå€™é€‰æ± ï¼š
        - æ¯ä¸ªå¤§ç±»åŒ…å« 3000 ä¸ª item_ids
        - æ¥è‡ª valid.json ä¸­å·²å‡ºç°çš„ item_ids + metadata ä¸­è¡¥å……
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        valid_path = dataset_dir / "nondisjoint" / "valid.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000
        TARGET_SIZE = 3000

        # âœ… åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ… æ„å»º category -> item_ids æ˜ å°„
        category_to_all_ids = defaultdict(list)
        for item_id, item in metadata.items():
            cid = item[CATEGORY_KEY]
            category_to_all_ids[cid].append(item_id)

        # âœ… ç»Ÿè®¡æ¯ä¸ªç±»åˆ«æ•°é‡ï¼Œé€‰å‡ºå¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ… åŠ è½½ valid ä¸­å‡ºç°çš„ item_ids
        with open(valid_path, "r", encoding="utf-8") as f:
            valid_outfits = json.load(f)
        valid_item_ids = {item_id for outfit in valid_outfits for item_id in outfit["item_ids"]}

        # âœ… åˆ†ç±» valid ä¸­çš„ item_ids åˆ°å„å¤§ç±»
        category_to_valid_ids = defaultdict(set)
        for item_id in valid_item_ids:
            cid = metadata.get(item_id, {}).get(CATEGORY_KEY)
            if cid in large_categories:
                category_to_valid_ids[cid].add(item_id)

        # âœ… æ„å»ºå€™é€‰æ± 
        candidate_pool = dict()
        for cid in large_categories:
            valid_ids = list(category_to_valid_ids.get(cid, set()))
            extra_ids = list(set(category_to_all_ids[cid]) - set(valid_ids))
            random.shuffle(extra_ids)  # éšæœºè¡¥é½
            total_ids = valid_ids + extra_ids[:max(0, TARGET_SIZE - len(valid_ids))]
            if len(total_ids) < TARGET_SIZE:
                print(f"âš ï¸ ç±»åˆ« {cid} æ— æ³•å‡‘æ»¡ {TARGET_SIZE} ä¸ªï¼ˆä»… {len(total_ids)} ä¸ªï¼‰")
            candidate_pool[cid] = total_ids[:TARGET_SIZE]  # ç²¾ç¡®æˆªæ–­

        # âœ… è¾“å‡ºç»Ÿè®¡
        print("\nğŸ“¦ å€™é€‰æ± æ„å»ºå®Œæ¯•ï¼ˆæ¯ç±» 3000 ä¸ª item_idsï¼‰")
        for cid, items in candidate_pool.items():
            print(f"ç±»åˆ« {cid}: {len(items)} items")

        # # âœ… å¦‚æœ‰éœ€è¦ï¼Œå¯å†™å…¥æ–‡ä»¶
        # # with open("candidate_pools.json", "w", encoding="utf-8") as f:
        # #     json.dump(candidate_pools, f, ensure_ascii=False, indent=2)
        #
        # return candidate_pools

class TestValidDataset(TestCase):
    def test_valid_dataset(self):
        """
        é—®é¢˜ï¼švalidä¸­çš„å¤§ç±»å…¨éƒ¨å°äº3000??
        ç»“è®ºï¼šyes
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        test_path = dataset_dir / "nondisjoint" / "valid.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000

        # âœ…åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ…åŠ è½½ test outfits
        with open(test_path, "r", encoding="utf-8") as f:
            test_outfits = json.load(f)

        # âœ…ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…¨å±€æ•°é‡ï¼Œé€‰å‡ºå¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ…ç»Ÿè®¡ test ä¸­å¤§ç±»çš„æ•°é‡
        test_item_ids = {item_id for outfit in test_outfits for item_id in outfit["item_ids"]}
        test_category_counter = Counter()

        for item_id in test_item_ids:
            cid = metadata.get(item_id, {}).get(CATEGORY_KEY)
            if cid in large_categories:
                test_category_counter[cid] += 1

        # âœ…è¾“å‡º
        print(f"\nğŸ“Š Valid ä¸­å¤§ç±»åˆ†å¸ƒï¼ˆè¶…è¿‡ 3000ï¼Ÿï¼‰")
        print(f"{'Category ID':>12s} | {'Test Count':>10s} | {'Needs Fill':>10s}")
        print("-" * 40)
        for cid in sorted(large_categories, key=lambda x: -test_category_counter[x]):
            count = test_category_counter[cid]
            need_fill = "âŒ No" if count >= THRESHOLD else "âœ… Yes"
            print(f"{cid:>12} | {count:>10} | {need_fill:>10}")

    def test_outfit_contains_large_category(self):
        """
        æµ‹è¯• valid.json ä¸­æ¯ä¸ª outfit çš„ item_ids æ˜¯å¦åŒ…å«å¤§ç±»
        è¾“å‡ºåŒ…å«å¤§ç±»çš„ä¸ªæ•°å’Œå®Œå…¨ä¸åŒ…å«å¤§ç±»çš„ä¸ªæ•°
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        valid_path = dataset_dir / "nondisjoint" / "valid.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000

        # âœ…åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ…åŠ è½½ valid outfits
        with open(valid_path, "r", encoding="utf-8") as f:
            valid_outfits = json.load(f)

        # âœ…ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…¨å±€æ•°é‡ï¼Œé€‰å‡ºå¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ…ç»Ÿè®¡æ¯ä¸ª outfit æ˜¯å¦åŒ…å«å¤§ç±»
        contains_large_count = 0
        not_contains_large_count = 0

        for outfit in valid_outfits:
            item_ids = outfit.get("item_ids", [])
            has_large = False
            for item_id in item_ids:
                category_id = metadata.get(item_id, {}).get(CATEGORY_KEY)
                if category_id in large_categories:
                    has_large = True
                    break
            if has_large:
                contains_large_count += 1
            else:
                not_contains_large_count += 1

        # âœ…è¾“å‡º
        print("\nğŸ“Š Outfit ä¸­æ˜¯å¦åŒ…å«å¤§ç±»ç»Ÿè®¡")
        print(f"âœ… è‡³å°‘åŒ…å«ä¸€ä¸ªå¤§ç±»çš„ outfit æ•°é‡ï¼š{contains_large_count}")
        print(f"âŒ å®Œå…¨ä¸åŒ…å«å¤§ç±»çš„ outfit æ•°é‡ï¼š{not_contains_large_count}")

    def test_valid_covers_all_large_categories(self):
        """
        æ£€æŸ¥ valid.json æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰çš„å¤§ç±»ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ª itemï¼‰
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        valid_path = dataset_dir / "nondisjoint" / "valid.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000

        # âœ… åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ… ç»Ÿè®¡å¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ… åŠ è½½ valid item_ids
        with open(valid_path, "r", encoding="utf-8") as f:
            valid_outfits = json.load(f)
        valid_item_ids = {item_id for outfit in valid_outfits for item_id in outfit["item_ids"]}

        # âœ… æå– valid ä¸­å®é™…å‡ºç°çš„ç±»åˆ«ï¼ˆé™å¤§ç±»ï¼‰
        valid_categories = set()
        for item_id in valid_item_ids:
            cid = metadata.get(item_id, {}).get(CATEGORY_KEY)
            if cid in large_categories:
                valid_categories.add(cid)

        # âœ… æ¯”è¾ƒå·®é›†
        uncovered_categories = large_categories - valid_categories

        # âœ… è¾“å‡º
        print(f"\nğŸ“Š å¤§ç±»æ€»æ•°ï¼š{len(large_categories)}")
        print(f"âœ… valid ä¸­å‡ºç°çš„å¤§ç±»ç§ç±»æ•°ï¼š{len(valid_categories)}")
        if uncovered_categories:
            print(f"âŒ ä»¥ä¸‹å¤§ç±»æ²¡æœ‰åœ¨ valid ä¸­å‡ºç°ï¼š{sorted(uncovered_categories)}")
        else:
            print("ğŸ‰ valid ä¸­è¦†ç›–äº†å…¨éƒ¨å¤§ç±»ï¼")

class TestTestDataset(TestCase):
    def test_test_dataset(self):
        """
        é—®é¢˜ï¼štestä¸­çš„å¤§ç±»å…¨éƒ¨å°äº3000??
        ç»“è®ºï¼šyes
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        test_path = dataset_dir / "nondisjoint" / "test.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000

        # âœ…åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ…åŠ è½½ test outfits
        with open(test_path, "r", encoding="utf-8") as f:
            test_outfits = json.load(f)

        # âœ…ç»Ÿè®¡æ¯ä¸ªç±»åˆ«çš„å…¨å±€æ•°é‡ï¼Œé€‰å‡ºå¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ…ç»Ÿè®¡ test ä¸­å¤§ç±»çš„æ•°é‡
        test_item_ids = {item_id for outfit in test_outfits for item_id in outfit["item_ids"]}
        test_category_counter = Counter()

        for item_id in test_item_ids:
            cid = metadata.get(item_id, {}).get(CATEGORY_KEY)
            if cid in large_categories:
                test_category_counter[cid] += 1

        # âœ…è¾“å‡º
        print(f"\nğŸ“Š Test ä¸­å¤§ç±»åˆ†å¸ƒï¼ˆè¶…è¿‡ 3000ï¼Ÿï¼‰")
        print(f"{'Category ID':>12s} | {'Test Count':>10s} | {'Needs Fill':>10s}")
        print("-" * 40)
        for cid in sorted(large_categories, key=lambda x: -test_category_counter[x]):
            count = test_category_counter[cid]
            need_fill = "âŒ No" if count >= THRESHOLD else "âœ… Yes"
            print(f"{cid:>12} | {count:>10} | {need_fill:>10}")

    def test_test_covers_all_large_categories(self):
        """
        æ£€æŸ¥ test.json æ˜¯å¦è¦†ç›–äº†æ‰€æœ‰çš„å¤§ç±»ï¼ˆè‡³å°‘åŒ…å«ä¸€ä¸ª itemï¼‰
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        metadata_path = dataset_dir / "item_metadata.json"
        test_path = dataset_dir / "nondisjoint" / "test.json"
        CATEGORY_KEY = "category_id"
        THRESHOLD = 3000

        # âœ… åŠ è½½ metadata
        with open(metadata_path, "r", encoding="utf-8") as f:
            raw_list = json.load(f)
            metadata = {item["item_id"]: item for item in raw_list}

        # âœ… ç»Ÿè®¡å¤§ç±»
        category_counts = Counter(item[CATEGORY_KEY] for item in metadata.values())
        large_categories = {cat for cat, count in category_counts.items() if count >= THRESHOLD}

        # âœ… åŠ è½½ test item_ids
        with open(test_path, "r", encoding="utf-8") as f:
            test_outfits = json.load(f)
        test_item_ids = {item_id for outfit in test_outfits for item_id in outfit["item_ids"]}

        # âœ… æå– test ä¸­å®é™…å‡ºç°çš„ç±»åˆ«ï¼ˆé™å¤§ç±»ï¼‰
        test_categories = set()
        for item_id in test_item_ids:
            cid = metadata.get(item_id, {}).get(CATEGORY_KEY)
            if cid in large_categories:
                test_categories.add(cid)

        # âœ… æ¯”è¾ƒå·®é›†
        uncovered_categories = large_categories - test_categories

        # âœ… è¾“å‡º
        print(f"\nğŸ“Š å¤§ç±»æ€»æ•°ï¼š{len(large_categories)}")
        print(f"âœ… test ä¸­å‡ºç°çš„å¤§ç±»ç§ç±»æ•°ï¼š{len(test_categories)}")
        if uncovered_categories:
            print(f"âŒ ä»¥ä¸‹å¤§ç±»æ²¡æœ‰åœ¨ test ä¸­å‡ºç°ï¼š{sorted(uncovered_categories)}")
        else:
            print("ğŸ‰ test ä¸­è¦†ç›–äº†å…¨éƒ¨å¤§ç±»ï¼")

class TestTrainAndTestDataset(TestCase):
    def test_train_and_test_dataset(self):
        """
        train and test åœ¨item_idçº§åˆ«æ˜¯å¦æœ‰é‡åˆ
        :return:
        """
        dataset_dir = ROOT_DIR / 'datasets' / 'polyvore'
        train_path = dataset_dir / "nondisjoint" / "train.json"
        test_path = dataset_dir / "nondisjoint" / "test.json"
        with open(train_path, 'r', encoding='utf-8') as f:
            train_outfits = json.load(f)
        with open(test_path, 'r', encoding='utf-8') as f:
            test_outfits = json.load(f)

        # æ”¶é›†æ‰€æœ‰ item_id
        train_ids = {iid for outfit in train_outfits for iid in outfit['item_ids']}
        test_ids = {iid for outfit in test_outfits for iid in outfit['item_ids']}

        # è®¡ç®—äº¤é›†
        overlap = train_ids & test_ids

        print(f"âœ… Train é›† item æ•°é‡: {len(train_ids)}")
        print(f"âœ… Test  é›† item æ•°é‡: {len(test_ids)}")
        print(f"ğŸ”¥ Train/Test é‡åˆ item æ•°é‡: {len(overlap)}")
        if overlap:
            print("ğŸŒŸ é‡åˆç¤ºä¾‹ï¼ˆæœ€å¤š 10 ä¸ªï¼‰ï¼š")
            for iid in list(overlap)[:10]:
                print("   -", iid)

    def test_train_pos_and_test_pos(self):
        def collect_pos_ids(polyvore_type: str, mode: str):
            ds = PolyvoreComplementaryItemRetrievalDataset(
                polyvore_type=polyvore_type,
                mode=mode,
                metadata=None,  # ä¼šåœ¨çˆ¶ç±»é‡Œè‡ªåŠ¨åŠ è½½
                embedding_dict=None,  # ç”¨ä¸åˆ° embedding_dict
                load_image=False
            )
            pos_ids = set()
            # cir_dataset æ¯ä¸ª entry éƒ½æœ‰ item_ids å’Œ positive_idx_list
            for entry in ds.cir_dataset:
                item_ids = entry['item_ids']
                for idx in entry['positive_idx_list']:
                    pos_ids.add(item_ids[idx])
            return pos_ids

        train_pos = collect_pos_ids('disjoint', 'train')
        test_pos = collect_pos_ids('disjoint', 'test')

        overlap = train_pos & test_pos

        print(f"âœ… Train æ­£æ ·æœ¬æ•°: {len(train_pos)}")
        print(f"âœ… Test  æ­£æ ·æœ¬æ•°: {len(test_pos)}")
        print(f"ğŸ”¥ é‡åˆæ­£æ ·æœ¬æ•°: {len(overlap)}")
        if overlap:
            print("ğŸŒŸ ç¤ºä¾‹é‡åˆ item_idï¼ˆæœ€å¤š10ä¸ªï¼‰ï¼š")
            for iid in list(overlap)[:10]:
                print(f"   - {iid}")